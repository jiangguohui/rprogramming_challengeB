---
title: "challengeB"
author: "Guohui Jiang; Jiayin Zhai"
date: "11/25/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Task 1B - Predicting house price in Ames, Lowa (continued)
## Step 1
The intuition behind Random Forests: we randomly split the training data into some subsamples, then we will use models to predict the observations of each subsample. The final prediction should be a function of each prediction. For example, we have 10,000 observations in the training data, then we randomly divide it into 100 datasets with 100 oberservations. We will run a model on one subsample. It will repeat the process 100 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction, or we can pick final prediction based on majority rule.

## Step 2
```{r, include=FALSE}
# importing training data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
library(tidyverse)

# remove variables with a lot of missing values
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist
train <- train %>% select(- one_of(remove.vars))

# for remaining variables, only keep rows without NAs
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

# make sure it's all clean : Yes
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

# running linear regression model
lm_model_1 <- lm(SalePrice ~ ., data= train)
summary(lm_model_1)

sum_lm_model_1 <- summary(lm_model_1)$coefficients #take only the table of coefficients and t stats and pvalues
class(sum_lm_model_1) #is a matrix
significant.vars <- row.names(sum_lm_model_1[sum_lm_model_1[,4] <= 0.01,]) #sum_lm_model_1[,4] is the p-value of each coefficient, here then i choose the variables that have coefficients significant at the 1% level

# choose any selection of such variables and run a more parcimonious model
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)

# prediction of linear model
prediction_lm <-as.data.frame(predict(lm_model_2, test, type="response"))
```

```{r, include=FALSE}
library(randomForest)
library(caret)
# setting the seed
set.seed(7)
# train the data using Random Forests technique
rf <- randomForest(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
```

```{r}
rf
# only show the result of random forests technique
```
## Step 3

```{r}
# making predictions on the test data using Random Forests
prediction_rf <- predict(rf, test)
# cobining the predictions from the two models
predictions <- cbind(prediction_lm, prediction_rf)
names(predictions) <- c("prediction_lm", "prediction_rf")
```

# Task 3B - Privacy regulation compliance in France

## Step 1
```{r}
library(data.table)
cnil <- fread('https://www.data.gouv.fr/s/resources/correspondants-informatique-et-libertes-cil/20171115-183631/OpenCNIL_Organismes_avec_CIL_VD_20171115.csv')
head(cnil)
```

## Step 2
```{r}
# getting the code of department each company belongs to through the first two digits of post code
library(tidyverse)
cnil$depart_code <- substr(cnil$Code_Postal, start = 1, stop = 2)

# calculating the number of organizations with CNIL per department
cnil %>% count(depart_code)
```
## Step 3
Before importing the large SIREN dataset into R, you need to download it into your computer.  
```{r}
name <- "sirc-17804_9075_14209_201710_L_M_20171101_030132835.csv"
## First we import the first 500 rows to check the large dataset
data <- fread(input = name, nrows = 500, header = TRUE)
dim(data)
colnames(data)
# There are 100 variables in this large dataset. actually, we do not need so much information in the following tasks. To finish Step 4, we only need the information of employment size, which is denoted by "TEFET" according to 'http://www.sirene.fr/sirene/public/variable/tefet'. Also, we need the variable "SIREN" and "DATEMAJ". Therefore, we only select the two columns to import.

## import the dataset formally
col.name <- c('SIREN', 'TEFET', 'DATEMAJ') 
#define the names of columns we will select
siren <- fread(input = name, header = TRUE, select = col.name)
#import the dataset within around 2 mins in my computer.

#siren_new <- siren %>% group_by(SIREN) %>% #slice(which.max(as.Date(DATEMAJ, '%Y-%m-%d')))

names(siren)[1] <- 'Siren'
class(cnil$Siren)
class(siren$Siren)
siren$Siren <- as.integer(siren$Siren)
combination <- left_join(cnil, siren, by='Siren')
head(combination)
# combining the two datasets
```
## Step 4 Plotting
```{r}
ggplot(data = combination,mapping = aes(x = as.factor(TEFET))) + geom_bar() + labs(x= "Tranche of salaried employees of the establishment", y='Count')

```
You should notice that: 
NN	Non-employer units  
00	0 employees  
01	1 or 2 employees  
02	3 to 5 employees  
03	6 to 9 employees  
11	10 to 19 employees  
12	20 to 49 employees  
21	50 to 99 employees  
22	100 to 199 employees  
31	200 to 249 employees  
32	250 to 499 employees  
41	500 to 999 employees  
42	1,000 to 1,999 employees  
51	2,000 to 4,999 employees  
52	5,000 to 9,999 employees  
53	10,000 employees and more  

